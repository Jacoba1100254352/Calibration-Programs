Index: Workflow_Programs/Neural_Fit.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\nfrom sklearn.preprocessing import StandardScaler\n\nfrom Configuration_Variables import *\nfrom Supplemental_Sensor_Graph_Functions import *\n\n\ndef load_and_prepare_data(sensor_num, test_num, bit_resolution, mapping='ADC_vs_N'):\n\t# Load data for the current test\n\tinstron_data = pd.read_csv(get_data_filepath(ALIGNED_INSTRON_DIR, sensor_num, _TEST_NUM=test_num))\n\tarduino_data = pd.read_csv(get_data_filepath(CALIBRATED_ARDUINO_DIR, sensor_num, _TEST_NUM=test_num))\n\t\n\tmin_length = min(len(instron_data), len(arduino_data))\n\tinstron_force = instron_data[\"Force [N]\"].iloc[:min_length].values.reshape(-1, 1)\n\tsensor_adc = arduino_data[f\"ADC{sensor_num}\"].iloc[:min_length].values.reshape(-1, 1)\n\t\n\t# Optional quantization step (if needed)\n\tinstron_force_quantized = quantize_data(instron_force.flatten(), bit_resolution)\n\tsensor_adc_quantized = quantize_data(sensor_adc.flatten(), bit_resolution)\n\t\n\t# Depending on the mapping, set inputs and targets\n\tif mapping == 'ADC_vs_N':\n\t\tinputs = instron_force_quantized.reshape(-1, 1)  # Instron force as input\n\t\ttargets = sensor_adc_quantized.reshape(-1, 1)  # Raw ADC values as targets\n\telif mapping == 'N_vs_N':\n\t\tinputs = sensor_adc_quantized.reshape(-1, 1)  # ADC values as input\n\t\ttargets = instron_force_quantized.reshape(-1, 1)  # Instron force as target\n\telse:\n\t\traise ValueError(\"Invalid mapping type. Use 'ADC_vs_N' or 'N_vs_N'.\")\n\t\n\treturn inputs, targets, instron_force_quantized, sensor_adc_quantized\n\n\ndef train_model_with_hyperparameter_tuning(inputs, targets, bit_resolution, test_num, hyperparams_dict):\n\tfrom sklearn.model_selection import train_test_split\n\timport itertools\n\t\n\t# Unpack hyperparameter lists\n\tunits_list = hyperparams_dict.get('units_list', [64, 128])\n\tlayers_list = hyperparams_dict.get('layers_list', [1, 2])\n\tactivation_list = hyperparams_dict.get('activation_list', ['tanh'])\n\tdropout_rate_list = hyperparams_dict.get('dropout_rate_list', [0.0, 0.1])\n\tl2_reg_list = hyperparams_dict.get('l2_reg_list', [0.0001, 0.001])\n\tlearning_rate_list = hyperparams_dict.get('learning_rate_list', [0.0005, 0.001])\n\tepochs_list = hyperparams_dict.get('epochs_list', [100])\n\tbatch_size_list = hyperparams_dict.get('batch_size_list', [64, 256])\n\t\n\t# Split the data into training and validation sets\n\tX_train, X_val, y_train, y_val = train_test_split(inputs, targets, test_size=0.2, random_state=42)\n\t\n\t# Scale the data\n\tinput_scaler = StandardScaler()\n\toutput_scaler = StandardScaler()\n\tX_train_scaled = input_scaler.fit_transform(X_train)\n\ty_train_scaled = output_scaler.fit_transform(y_train)\n\tX_val_scaled = input_scaler.transform(X_val)\n\ty_val_scaled = output_scaler.transform(y_val)\n\t\n\t# Define the hyperparameter grid\n\thyperparameter_grid = list(itertools.product(\n\t\tunits_list, layers_list, activation_list,\n\t\tdropout_rate_list, l2_reg_list, learning_rate_list,\n\t\tepochs_list, batch_size_list\n\t))\n\t\n\tbest_val_loss = float('inf')\n\tbest_hyperparams = None\n\tbest_model_state = None\n\t\n\tresults_list = []\n\t\n\t# Hyperparameter tuning\n\tfor (units_, layers_, activation_, dropout_rate_, l2_reg_, learning_rate_, epochs_, batch_size_) in hyperparameter_grid:\n\t\tprint(f\"Test {test_num}, Hyperparameters: units={units_}, layers={layers_}, activation={activation_}, dropout_rate={dropout_rate_}, l2_reg={l2_reg_}, learning_rate={learning_rate_}, epochs={epochs_}, batch_size={batch_size_}\")\n\t\t\n\t\t# Initialize the model\n\t\tmodel = QuantizedNN(\n\t\t\tinput_dim=1, units=units_, layers=layers_, activation=activation_, dropout_rate=dropout_rate_,\n\t\t\tweight_bit_width=bit_resolution, act_bit_width=bit_resolution\n\t\t)\n\t\t\n\t\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\t\tmodel.to(device)\n\t\tcriterion = nn.MSELoss()\n\t\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_, weight_decay=l2_reg_)\n\t\t\n\t\t# Convert data to PyTorch tensors for training\n\t\tX_train_tensor = torch.Tensor(X_train_scaled).to(device)\n\t\ty_train_tensor = torch.Tensor(y_train_scaled).to(device)\n\t\tX_val_tensor = torch.Tensor(X_val_scaled).to(device)\n\t\ty_val_tensor = torch.Tensor(y_val_scaled).to(device)\n\t\t\n\t\ttrain_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n\t\ttrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_, shuffle=True)\n\t\t\n\t\t# Train the model\n\t\tfor epoch in range(epochs_):\n\t\t\tmodel.train()\n\t\t\ttotal_loss = 0\n\t\t\tfor x_batch, y_batch in train_dataloader:\n\t\t\t\toptimizer.zero_grad()\n\t\t\t\toutputs = model(x_batch)\n\t\t\t\tloss = criterion(outputs, y_batch)\n\t\t\t\tloss.backward()\n\t\t\t\toptimizer.step()\n\t\t\t\ttotal_loss += loss.item()\n\t\t\n\t\t# Validation\n\t\tmodel.eval()\n\t\twith torch.no_grad():\n\t\t\tval_outputs = model(X_val_tensor)\n\t\t\tval_loss = criterion(val_outputs, y_val_tensor).item()\n\t\t\n\t\t# Save results\n\t\tresults_list.append({\n\t\t\t'units': units_,\n\t\t\t'layers': layers_,\n\t\t\t'activation': activation_,\n\t\t\t'dropout_rate': dropout_rate_,\n\t\t\t'l2_reg': l2_reg_,\n\t\t\t'learning_rate': learning_rate_,\n\t\t\t'epochs': epochs_,\n\t\t\t'batch_size': batch_size_,\n\t\t\t'val_loss': val_loss\n\t\t})\n\t\t\n\t\tif val_loss < best_val_loss:\n\t\t\tbest_val_loss = val_loss\n\t\t\tbest_hyperparams = {\n\t\t\t\t'units': units_,\n\t\t\t\t'layers': layers_,\n\t\t\t\t'activation': activation_,\n\t\t\t\t'dropout_rate': dropout_rate_,\n\t\t\t\t'l2_reg': l2_reg_,\n\t\t\t\t'learning_rate': learning_rate_,\n\t\t\t\t'epochs': epochs_,\n\t\t\t\t'batch_size': batch_size_\n\t\t\t}\n\t\t\tbest_model_state = model.state_dict()\n\t\n\t# Retrain the best model on the full dataset\n\tmodel = QuantizedNN(\n\t\tinput_dim=1, units=best_hyperparams['units'], layers=best_hyperparams['layers'],\n\t\tactivation=best_hyperparams['activation'], dropout_rate=best_hyperparams['dropout_rate'],\n\t\tweight_bit_width=bit_resolution, act_bit_width=bit_resolution\n\t)\n\tmodel.load_state_dict(best_model_state)\n\treturn model, input_scaler, output_scaler, best_hyperparams\n\n\ndef train_model(inputs, targets, units, layers, activation, dropout_rate, l2_reg, learning_rate, epochs, batch_size, bit_resolution):\n\t# Initialize scalers\n\tinput_scaler = StandardScaler()\n\toutput_scaler = StandardScaler()\n\tinputs_scaled = input_scaler.fit_transform(inputs)\n\ttargets_scaled = output_scaler.fit_transform(targets)\n\t\n\t# Initialize and train the quantized neural network\n\tmodel = QuantizedNN(\n\t\tinput_dim=1, units=units, layers=layers, activation=activation, dropout_rate=dropout_rate,\n\t\tweight_bit_width=bit_resolution, act_bit_width=bit_resolution\n\t)\n\t\n\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\tmodel.to(device)\n\tcriterion = nn.MSELoss()\n\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2_reg)\n\t\n\t# Convert data to PyTorch tensors for training\n\tinputs_tensor = torch.Tensor(inputs_scaled).to(device)\n\ttargets_tensor = torch.Tensor(targets_scaled).to(device)\n\t\n\tdataset = torch.utils.data.TensorDataset(inputs_tensor, targets_tensor)\n\tdataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\t\n\t# Train the model\n\tfor epoch in range(epochs):\n\t\tmodel.train()\n\t\ttotal_loss = 0\n\t\tfor x_batch, y_batch in dataloader:\n\t\t\toptimizer.zero_grad()\n\t\t\toutputs = model(x_batch)\n\t\t\tloss = criterion(outputs, y_batch)\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\ttotal_loss += loss.item()\n\t\tif (epoch + 1) % 10 == 0:\n\t\t\tprint(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / len(dataloader):.6f}\")\n\t\n\treturn model, input_scaler, output_scaler\n\n\ndef evaluate_model(model, inputs, instron_force, sensor_adc, input_scaler, output_scaler, mapping):\n\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\tmodel.eval()\n\twith torch.no_grad():\n\t\tinputs_scaled = input_scaler.transform(inputs)\n\t\tinputs_tensor = torch.Tensor(inputs_scaled).to(device)\n\t\toutputs_scaled = model(inputs_tensor).cpu().numpy()\n\t\toutputs = output_scaler.inverse_transform(outputs_scaled)\n\t\n\t# First Graph: Residuals in N (calibrated sensor N - Instron N)\n\tif mapping == 'N_vs_N':\n\t\t# Residuals are calculated as the difference between the calibrated N and Instron N\n\t\tresiduals = outputs.flatten() - instron_force.flatten()\n\t\treturn outputs.flatten(), residuals\n\t\n\t# Second Graph: Residuals in ADC (Instron N - ADC values)\n\telif mapping == 'ADC_vs_N':\n\t\t# Residuals are the difference between the ADC values and the predicted output from the model\n\t\tresiduals = sensor_adc.flatten() - outputs.flatten()\n\t\treturn outputs, residuals\n\n\ndef plot_overlay(overlay_ax, inputs, targets, outputs, test_num, mapping):\n\tif mapping == 'N_vs_N':\n\t\t# Plot calibrated N values (Y) vs Instron N (X)\n\t\tx = targets.flatten()  # Instron N\n\t\ty_pred = outputs.flatten()  # Predicted N (Calibrated)\n\t\txlabel = \"Instron Force [N]\"\n\t\tylabel = \"Calibrated Force [N]\"\n\telif mapping == 'ADC_vs_N':\n\t\t# Plot ADC vs Instron N\n\t\tx = targets.flatten()  # Instron N\n\t\ty_pred = outputs.flatten()  # Predicted ADC\n\t\txlabel = \"Instron Force [N]\"\n\t\tylabel = \"ADC Value\"\n\telse:\n\t\traise ValueError(\"Invalid mapping type. Use 'ADC_vs_N' or 'N_vs_N'.\")\n\t\n\toverlay_ax.plot(x, y_pred, label=f\"Test {test_num} - Neural Fit\", linewidth=2)\n\toverlay_ax.set_xlabel(xlabel)\n\toverlay_ax.set_ylabel(ylabel)\n\toverlay_ax.grid(True)\n\n\ndef plot_residuals(residuals_ax, instron_force, residuals, test_num, mapping):\n\tx = instron_force.flatten()  # Instron Force (N) is the baseline\n\t\n\t# Invert the x-axis to make the direction go from larger to smaller\n\tresiduals_ax.invert_xaxis()\n\t\n\tif mapping == 'N_vs_N':\n\t\t# First graph: Residuals in N vs Instron N\n\t\tresiduals_ax.plot(x, residuals, label=f\"Residuals [N] (Test {test_num})\", linewidth=2)  # (Test {test_num})\n\t\tresiduals_ax.set_xlabel(\"Instron Force [N]\")\n\t\tresiduals_ax.set_ylabel(\"Residuals [N]\")\n\telif mapping == 'ADC_vs_N':\n\t\t# Second graph: Residuals in ADC vs Instron N\n\t\tresiduals_ax.plot(x, residuals, label=f\"Residuals [ADC] (Test {test_num})\", linewidth=2)  # (Test {test_num})\n\t\tresiduals_ax.set_xlabel(\"Instron Force [N]\")\n\t\tresiduals_ax.set_ylabel(\"Residuals [ADC]\")\n\telse:\n\t\traise ValueError(\"Invalid mapping type. Use 'N_vs_N' or 'ADC_vs_N'.\")\n\tresiduals_ax.grid(True)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Workflow_Programs/Neural_Fit.py b/Workflow_Programs/Neural_Fit.py
--- a/Workflow_Programs/Neural_Fit.py	(revision 5ae8c20848b87cd770d30441da1385f31aed0dbe)
+++ b/Workflow_Programs/Neural_Fit.py	(date 1727746091888)
@@ -18,13 +18,15 @@
 	instron_force_quantized = quantize_data(instron_force.flatten(), bit_resolution)
 	sensor_adc_quantized = quantize_data(sensor_adc.flatten(), bit_resolution)
 	
-	# Depending on the mapping, set inputs and targets
+	# Combine Instron force and sensor data into a single input array
+	inputs = np.column_stack((instron_force_quantized.reshape(-1, 1), sensor_adc_quantized.reshape(-1, 1)))
+	
 	if mapping == 'ADC_vs_N':
-		inputs = instron_force_quantized.reshape(-1, 1)  # Instron force as input
-		targets = sensor_adc_quantized.reshape(-1, 1)  # Raw ADC values as targets
+		# Instron force as target
+		targets = sensor_adc_quantized.reshape(-1, 1)
 	elif mapping == 'N_vs_N':
-		inputs = sensor_adc_quantized.reshape(-1, 1)  # ADC values as input
-		targets = instron_force_quantized.reshape(-1, 1)  # Instron force as target
+		# Instron force as target
+		targets = instron_force_quantized.reshape(-1, 1)
 	else:
 		raise ValueError("Invalid mapping type. Use 'ADC_vs_N' or 'N_vs_N'.")
 	
@@ -155,9 +157,12 @@
 	inputs_scaled = input_scaler.fit_transform(inputs)
 	targets_scaled = output_scaler.fit_transform(targets)
 	
+	# Determine the correct input dimension
+	input_dim = inputs.shape[1]  # This will be 2 in your case
+	
 	# Initialize and train the quantized neural network
 	model = QuantizedNN(
-		input_dim=1, units=units, layers=layers, activation=activation, dropout_rate=dropout_rate,
+		input_dim=input_dim, units=units, layers=layers, activation=activation, dropout_rate=dropout_rate,
 		weight_bit_width=bit_resolution, act_bit_width=bit_resolution
 	)
 	
Index: Workflow_Programs/Supplemental_Sensor_Graph_Functions.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import brevitas.nn as qnn\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_model_optimization as tfmot\nimport torch.nn as nn\nfrom brevitas.core.scaling import ScalingImplType\nfrom brevitas.quant import Int8WeightPerTensorFixedPoint\nfrom keras.layers import BatchNormalization, Dense, Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2\nfrom keras.utils import plot_model\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom scipy.signal import medfilt, savgol_filter\nfrom scipy.stats import linregress\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\nclass QuantizedNN(nn.Module):\n\tdef __init__(\n\t\tself, input_dim, units=64, layers=2, activation='relu', dropout_rate=0.5,\n\t\tweight_bit_width=8, act_bit_width=8\n\t):\n\t\tsuper(QuantizedNN, self).__init__()\n\t\t\n\t\t# Define activations\n\t\tactivation_functions = {\n\t\t\t'relu': qnn.QuantReLU,\n\t\t\t'tanh': qnn.QuantTanh,\n\t\t\t'sigmoid': qnn.QuantSigmoid,\n\t\t\t'hardtanh': qnn.QuantHardTanh,\n\t\t\t'identity': qnn.QuantIdentity\n\t\t}\n\t\t\n\t\tquant_activation_class = activation_functions.get(activation.lower(), qnn.QuantReLU)\n\t\t\n\t\t# Input layer with automatic quantization scaling\n\t\tself.layers = nn.ModuleList([\n\t\t\tqnn.QuantLinear(\n\t\t\t\tinput_dim, units,\n\t\t\t\tbias=True,\n\t\t\t\tweight_bit_width=weight_bit_width,\n\t\t\t\tweight_quant=Int8WeightPerTensorFixedPoint,\n\t\t\t\tscaling_impl_type=ScalingImplType.STATS\n\t\t\t)\n\t\t])\n\t\tself.activations = nn.ModuleList([quant_activation_class(bit_width=act_bit_width)])\n\t\tself.dropouts = nn.ModuleList([nn.Dropout(dropout_rate)])\n\t\t\n\t\t# Hidden layers\n\t\tfor _ in range(1, layers):\n\t\t\tself.layers.append(\n\t\t\t\tqnn.QuantLinear(\n\t\t\t\t\tunits, units,\n\t\t\t\t\tbias=True,\n\t\t\t\t\tweight_bit_width=weight_bit_width,\n\t\t\t\t\tweight_quant=Int8WeightPerTensorFixedPoint,\n\t\t\t\t\tscaling_impl_type=ScalingImplType.STATS\n\t\t\t\t)\n\t\t\t)\n\t\t\tself.activations.append(quant_activation_class(bit_width=act_bit_width))\n\t\t\tself.dropouts.append(nn.Dropout(dropout_rate))\n\t\t\n\t\t# Output layer\n\t\tself.output_layer = qnn.QuantLinear(\n\t\t\tunits, 1,\n\t\t\tbias=True,\n\t\t\tweight_bit_width=weight_bit_width,\n\t\t\tweight_quant=Int8WeightPerTensorFixedPoint,\n\t\t\tscaling_impl_type=ScalingImplType.STATS\n\t\t)\n\t\n\tdef forward(self, x):\n\t\tfor layer, activation, dropout in zip(self.layers, self.activations, self.dropouts):\n\t\t\tx = layer(x)\n\t\t\tx = activation(x)\n\t\t\tx = dropout(x)\n\t\tx = self.output_layer(x)\n\t\treturn x\n\n\ndef avg(lst):\n\treturn sum(lst) / len(lst)\n\n\ndef difference_polarity(lst1, lst2):\n\treturn (avg(lst1) - avg(lst2)) / abs(avg(lst1) - avg(lst2))\n\n\ndef calculate_line_of_best_fit(x, y, isPolyfit=False, order=1):\n\tif not isPolyfit:\n\t\tslope_avg, intercept_avg, _, _, _ = linregress(x, y)\n\t\tline_of_best_fit = slope_avg * x + intercept_avg\n\telse:\n\t\tcoefficients = np.polyfit(x, y, order)\n\t\tpolynomial = np.poly1d(coefficients)\n\t\tline_of_best_fit = polynomial(x)\n\treturn line_of_best_fit\n\n\n# Function to apply smoothing to residuals\ndef apply_smoothing(residuals, method, window_size, poly_order):\n\t\"\"\"\n\tApply smoothing to the residuals using the specified method.\n\n\tParameters:\n\t- residuals: The residual data to be smoothed.\n\t- method: The smoothing method ('savgol', 'boxcar', 'median', or None).\n\t- window_size: The window size for the smoothing operation.\n\t- poly_order: The polynomial order for Savitzky-Golay filter (only used if method is 'savgol').\n\n\tReturns:\n\t- smoothed_residuals: The smoothed residuals.\n\t\"\"\"\n\tif isinstance(residuals, pd.Series):\n\t\tresiduals = residuals.values.flatten()  # Convert Pandas Series to NumPy array and flatten\n\telse:\n\t\tresiduals = residuals.flatten()  # If it's already a NumPy array, just flatten it\n\t\n\tif method == 'savgol':\n\t\tif window_size is None:\n\t\t\traise ValueError(\"Window size must be specified for Savitzky-Golay smoothing.\")\n\t\tsmoothed_residuals = savgol_filter(residuals, window_length=window_size, polyorder=poly_order)\n\telif method == 'boxcar':\n\t\tif window_size is None:\n\t\t\traise ValueError(\"Window size must be specified for boxcar smoothing.\")\n\t\tsmoothed_residuals = np.convolve(residuals, np.ones(window_size) / window_size, mode='valid')\n\t\tsmoothed_residuals = np.pad(smoothed_residuals, (window_size // 2, window_size // 2), mode='edge')\n\t\tif len(smoothed_residuals) > len(residuals):\n\t\t\tsmoothed_residuals = smoothed_residuals[:len(residuals)]\n\t\telif len(smoothed_residuals) < len(residuals):\n\t\t\tsmoothed_residuals = np.pad(smoothed_residuals, (0, len(residuals) - len(smoothed_residuals)), 'edge')\n\telif method == 'median':\n\t\tif window_size is None:\n\t\t\traise ValueError(\"Window size must be specified for median filtering.\")\n\t\tif window_size % 2 == 0:  # Ensure window_size is odd\n\t\t\twindow_size += 1\n\t\tsmoothed_residuals = medfilt(residuals, kernel_size=window_size)\n\telse:\n\t\tsmoothed_residuals = residuals\n\t\n\treturn smoothed_residuals\n\n\n# Helper function for quantizing data (if using custom bit resolutions for inputs/outputs)\ndef quantize_data(data, bit_resolution):\n\t\"\"\"Quantize the data to the given bit resolution.\"\"\"\n\tmax_val = np.max(np.abs(data))\n\tscale = (2**bit_resolution) - 1\n\tquantized_data = np.round(data / max_val * scale) * max_val / scale\n\treturn quantized_data\n\n\n# Function to build a quantized neural network model\ndef build_quantized_neural_network(input_dim, layers=2, units=64, activation='relu', dropout_rate=0.5, l2_reg=0.01, learning_rate=0.001):\n\t\"\"\"\n\tBuild a neural network with quantization-aware training at 8-bit resolution.\n\t\"\"\"\n\t# Define the basic sequential model\n\tmodel = tf.keras.Sequential()\n\t\n\t# Add input and first layer\n\tmodel.add(tf.keras.layers.InputLayer(input_shape=(input_dim,)))\n\tmodel.add(tf.keras.layers.Dense(units, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n\tmodel.add(tf.keras.layers.Dropout(dropout_rate))\n\t\n\t# Add more layers as needed\n\tfor _ in range(layers - 1):\n\t\tmodel.add(tf.keras.layers.Dense(units, activation=activation, kernel_regularizer=tf.keras.regularizers.l2(l2_reg)))\n\t\tmodel.add(tf.keras.layers.Dropout(dropout_rate))\n\t\n\t# Output layer\n\tmodel.add(tf.keras.layers.Dense(1))  # Output layer for regression\n\t\n\t# Compile the model\n\tmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n\t\n\t# Apply 8-bit quantization-aware training\n\tquant_aware_model = tfmot.quantization.keras.quantize_model(model)\n\t\n\treturn quant_aware_model\n\n\n# Function to build a neural network model\ndef build_neural_network(input_dim, layers=2, units=64, activation='relu', dropout_rate=0.5, l2_reg=0.01, learning_rate=0.001):\n\t\"\"\"\n\tBuild a customizable neural network model with specified parameters.\n\n\tParameters:\n\t- input_dim: Dimension of the input data.\n\t- layers: Number of hidden layers in the neural network.\n\t- units: Number of units in each hidden layer.\n\t- activation: Activation function for hidden layers.\n\t- dropout_rate: Dropout rate for regularization.\n\t- l2_reg: L2 regularization parameter.\n\t- learning_rate: Learning rate for the optimizer.\n\n\tReturns:\n\t- model: Compiled Keras model.\n\t\"\"\"\n\t# Sequential allows for building on/adding layers\n\tmodel = Sequential()\n\t# Fully-Connected or Dense layer, all the neurons are connected to the next/previous layer\n\tmodel.add(Dense(units, input_dim=input_dim, activation=activation, kernel_regularizer=l2(l2_reg)))\n\tmodel.add(Dropout(dropout_rate))  # Dropout layer works by randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.\n\tmodel.add(BatchNormalization())  # Batch normalization works by normalizing the input layer by adjusting and scaling the activations.\n\t\n\t\"\"\"\n\t•\tL1 and L2 Regularization:\n\t\t•\tL2 Regularization (Ridge): Adds a penalty equal to the sum of the squared weights to the loss function (e.g., Dense(units, kernel_regularizer=l2(0.01))).\n\t\t•\tL1 Regularization (Lasso): Adds a penalty equal to the sum of the absolute values of the weights (e.g., Dense(units, kernel_regularizer=l1(0.01))).\n\t\t•\tElastic Net: Combines L1 and L2 regularization.\n\t\"\"\"\n\tfor _ in range(layers - 1):\n\t\tmodel.add(Dense(units, activation=activation, kernel_regularizer=l2(l2_reg)))\n\t\tmodel.add(Dropout(dropout_rate))\n\t\tmodel.add(BatchNormalization())\n\t\n\tmodel.add(Dense(1))  # Output layer for regression\n\tmodel.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n\t\n\treturn model\n\n\ndef get_model_memory_usage(batch_size, model):\n\timport numpy as np\n\t\n\ttry:\n\t\tfrom keras import backend as K\n\texcept:\n\t\tfrom tensorflow.keras import backend as K\n\t\n\tshapes_mem_count = 0\n\tinternal_model_mem_count = 0\n\tfor l in model.layers:\n\t\tlayer_type = l.__class__.__name__\n\t\tif layer_type == 'Model':\n\t\t\tinternal_model_mem_count += get_model_memory_usage(batch_size, l)\n\t\tsingle_layer_mem = 1\n\t\tout_shape = l.output_shape\n\t\tif type(out_shape) is list:\n\t\t\tout_shape = out_shape[0]\n\t\tfor s in out_shape:\n\t\t\tif s is None:\n\t\t\t\tcontinue\n\t\t\tsingle_layer_mem *= s\n\t\tshapes_mem_count += single_layer_mem\n\t\n\ttrainable_count = np.sum([K.count_params(p) for p in model.trainable_weights])\n\tnon_trainable_count = np.sum([K.count_params(p) for p in model.non_trainable_weights])\n\t\n\tnumber_size = 4.0\n\tif K.floatx() == 'float16':\n\t\tnumber_size = 2.0\n\tif K.floatx() == 'float64':\n\t\tnumber_size = 8.0\n\t\n\ttotal_memory = number_size * (batch_size * shapes_mem_count + trainable_count + non_trainable_count)\n\tgbytes = np.round(total_memory / (1024.0**3), 3) + internal_model_mem_count\n\treturn gbytes\n\n\ndef get_bit_resolution(model):\n\ttry:\n\t\tfrom keras import backend as K\n\texcept:\n\t\tfrom tensorflow.keras import backend as K\n\t\n\t# Checking default precision used by Keras backend\n\tfloatx_dtype = K.floatx()  # Returns 'float16', 'float32', or 'float64'\n\tbit_resolution = {'float16': 16, 'float32': 32, 'float64': 64}.get(floatx_dtype, 'Unknown')\n\t\n\tprint(f\"Model uses {bit_resolution}-bit precision for signals (weights, activations).\")\n\t\n\t# If needed, we can also check each layer's dtype individually:\n\tfor layer in model.layers:\n\t\tif hasattr(layer, 'dtype'):\n\t\t\tprint(f\"Layer {layer.name} uses {layer.dtype} precision\")\n\t\n\treturn bit_resolution\n\n\ndef get_quantized_bit_resolution(model):\n\ttry:\n\t\tfrom keras import backend as K\n\texcept:\n\t\tfrom tensorflow.keras import backend as K\n\t\n\tquantized_layers = []\n\t\n\t# Check each layer to determine if it's quantized\n\tfor layer in model.layers:\n\t\tlayer_type = layer.__class__.__name__\n\t\t\n\t\t# Layers commonly used in quantized models\n\t\tif 'Quant' in layer_type or hasattr(layer, 'quantize_config'):\n\t\t\tquantized_layers.append(layer)\n\t\t\tprint(f\"Layer {layer.name} is quantized.\")\n\t\n\tif len(quantized_layers) == 0:\n\t\tprint(\"No quantized layers found in the model.\")\n\t\treturn None\n\t\n\t# Assuming TensorFlow Model Optimization Toolkit or similar library is used\n\tfor layer in quantized_layers:\n\t\t# Example of retrieving bit resolution - this part depends on the exact quantization method\n\t\t# Check for a custom attribute like 'quant_bits' (you may need to adjust this based on your framework)\n\t\tif hasattr(layer, 'quant_bits'):\n\t\t\tprint(f\"Layer {layer.name} uses {layer.quant_bits}-bit precision\")\n\t\telse:\n\t\t\tprint(f\"Layer {layer.name} might use quantization, but bit precision is not directly accessible.\")\n\t\n\treturn quantized_layers\n\n\ndef determine_minimum_bit_resolution(data, precision=None, epsilon=1e-12):\n\t\"\"\"\n\tDetermine the minimum bit resolution required to represent the input data\n\tas precisely as possible.\n\n\tParameters:\n\t- data: NumPy array or list of input data to analyze.\n\t- precision: The required precision (smallest difference between values).\n\t\t\t\t If None, it will be automatically calculated based on data.\n\t- epsilon: A small value to handle floating-point precision issues.\n\n\tReturns:\n\t- A dictionary containing:\n\t\t- minimum_bits: The minimum number of bits required.\n\t\t- dynamic_range: The range of the data.\n\t\t- min_val: The minimum value in the data.\n\t\t- max_val: The maximum value in the data.\n\t\t- precision: The precision used for calculation.\n\t\"\"\"\n\t\n\t# Convert to NumPy array if input is a list\n\tif isinstance(data, list):\n\t\tdata = np.array(data)\n\t\n\t# Ensure data is a floating-point type for precision calculations\n\tdata = data.astype(np.float64)\n\t\n\t# Calculate range\n\tmin_val = np.min(data)\n\tmax_val = np.max(data)\n\tdynamic_range = max_val - min_val\n\t\n\t# Handle case where dynamic range is zero (all values are identical)\n\tif dynamic_range == 0:\n\t\tprint(\"All data points are identical. Minimum bit resolution is 1 bit.\")\n\t\treturn {\n\t\t\t\"minimum_bits\": 1,  # 1 bit is sufficient to represent a single unique value\n\t\t\t\"dynamic_range\": dynamic_range,\n\t\t\t\"min_val\": min_val,\n\t\t\t\"max_val\": max_val,\n\t\t\t\"precision\": 0  # No precision needed\n\t\t}\n\t\n\t# If precision is not provided, calculate it\n\tif precision is None:\n\t\tunique_values = np.unique(data)\n\t\tif len(unique_values) < 2:\n\t\t\t# Only one unique value exists\n\t\t\tprint(\"Only one unique value found in data. Minimum bit resolution is 1 bit.\")\n\t\t\treturn {\n\t\t\t\t\"minimum_bits\": 1,\n\t\t\t\t\"dynamic_range\": dynamic_range,\n\t\t\t\t\"min_val\": min_val,\n\t\t\t\t\"max_val\": max_val,\n\t\t\t\t\"precision\": 0\n\t\t\t}\n\t\t# Calculate the smallest difference between sorted unique values\n\t\tdiffs = np.diff(unique_values)\n\t\tprecision = np.min(diffs)\n\t\tif not np.isfinite(precision) or precision <= 0:\n\t\t\tprint(\"Calculated precision is non-finite or non-positive. Setting precision to epsilon.\")\n\t\t\tprecision = epsilon\n\t\n\t# Ensure precision is positive and finite\n\tif precision <= 0 or not np.isfinite(precision):\n\t\tprint(\"Precision must be positive and finite. Setting precision to epsilon.\")\n\t\tprecision = epsilon\n\t\n\t# Calculate the number of bits required\n\tratio = dynamic_range / precision\n\tif ratio <= 0:\n\t\tprint(\"Dynamic range divided by precision is non-positive. Setting minimum bits to 1.\")\n\t\tminimum_bits = 1\n\telse:\n\t\tlog_ratio = np.log2(ratio)\n\t\tif not np.isfinite(log_ratio):\n\t\t\tprint(\"Logarithm of ratio is non-finite. Setting minimum bits to 1.\")\n\t\t\tminimum_bits = 1\n\t\telse:\n\t\t\tminimum_bits = int(np.ceil(log_ratio))\n\t\n\treturn {\n\t\t\"minimum_bits\": minimum_bits,\n\t\t\"dynamic_range\": dynamic_range,\n\t\t\"min_val\": min_val,\n\t\t\"max_val\": max_val,\n\t\t\"precision\": precision\n\t}\n\n\n# Example usage of hyperparameter tuning with RandomizedSearchCV\ndef hyperparameter_tuning(X_train, y_train, input_dim):\n\tmodel = KerasRegressor(build_fn=build_neural_network, input_dim=input_dim, verbose=0)\n\t\n\tparam_dist = {\n\t\t'layers': [1, 2, 3],\n\t\t'units': [32, 64, 128],\n\t\t'activation': ['relu', 'tanh'],  # 'sigmoid'\n\t\t'dropout_rate': [0.2, 0.5, 0.7],\n\t\t'l2_reg': [0.01, 0.001, 0.0001],\n\t\t'learning_rate': [0.01, 0.001, 0.0001],\n\t\t'batch_size': [16, 32, 64, 128, 256],\n\t\t'epochs': [50, 100, 200]\n\t}\n\t\n\trandom_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=20, cv=3, verbose=2, scoring=make_scorer(mean_squared_error, greater_is_better=False))\n\t\n\trandom_search_result = random_search.fit(X_train, y_train)\n\t\n\t# Create a DataFrame to store all results\n\tresults_df = pd.DataFrame(random_search_result.cv_results_)\n\t\n\t# Extract relevant columns and sort by rank_test_score\n\tresults_df = results_df[['params', 'mean_test_score', 'rank_test_score']]\n\tsorted_results = results_df.sort_values(by='rank_test_score')\n\t\n\t# Display all sorted results\n\tprint(\"All Sorted Results:\")\n\tprint(sorted_results)\n\t\n\t# Display the best parameters and score\n\tprint(\"\\nBest Parameters:\", random_search_result.best_params_)\n\tprint(\"Best Score:\", random_search_result.best_score_)\n\t\n\treturn random_search_result.best_estimator_\n\n\ndef display_info(batch_size, model):\n\tmodel.summary(expand_nested=True, show_trainable=True)\n\tmodel_memory = get_model_memory_usage(batch_size, model)\n\tbit_resolution = get_bit_resolution(model)\n\tquantized_layers = get_quantized_bit_resolution(model)\n\tprint(f\"Total model memory usage: {model_memory} GB\")\n\tprint(f\"Bit resolution: {bit_resolution} bits\")\n\tprint(f\"Quantized layers: {quantized_layers}\")\n\t\n\t# Print the weights for each layer\n\tprint(\"Model Weights:\")\n\tfor layer in model.layers:\n\t\tweights = layer.get_weights()\n\t\t\n\t\tif len(weights) > 0:\n\t\t\tprint(f\"Layer: {layer.name}\")\n\t\t\t# Different layers have different numbers of components\n\t\t\tif isinstance(layer, tf.keras.layers.BatchNormalization):\n\t\t\t\tgamma, beta, moving_mean, moving_variance = weights\n\t\t\t\tprint(\"Gamma (scale):\", gamma)\n\t\t\t\tprint(\"Beta (shift):\", beta)\n\t\t\t\tprint(\"Moving Mean:\", moving_mean)\n\t\t\t\tprint(\"Moving Variance:\", moving_variance)\n\t\t\telif len(weights) == 2:  # Dense layers have weights and biases\n\t\t\t\tweights, biases = weights\n\t\t\t\tprint(\"Weights:\", weights)\n\t\t\t\tprint(\"Biases:\", biases)\n\t\t\tprint(\"-\" * 50)\n\n\ndef display_layer_info(model):\n\t# Create a plot of the model architecture\n\tplot_model(model, to_file=\"model_visual.png\", show_shapes=True, show_layer_names=True)\n\t\n\t# Display the generated plot\n\timg = plt.imread(\"model_visual.png\")\n\tplt.figure(figsize=(10, 10))\n\tplt.imshow(img)\n\tplt.axis('off')\n\tplt.show()\n\n\ndef calculate_bit_resolution(data_title, data):\n\t# Determine minimum bit resolution for the data\n\tprint(data_title + \":\")\n\tresult = determine_minimum_bit_resolution(data)\n\tprint(f\"Minimum Bit Resolution: {result['minimum_bits']} bits\")\n\tprint(f\"Dynamic Range: {result['dynamic_range']}\")\n\tprint(f\"Minimum Value: {result['min_val']}\")\n\tprint(f\"Maximum Value: {result['max_val']}\")\n\tprint(f\"Precision: {result['precision']}\")\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Workflow_Programs/Supplemental_Sensor_Graph_Functions.py b/Workflow_Programs/Supplemental_Sensor_Graph_Functions.py
--- a/Workflow_Programs/Supplemental_Sensor_Graph_Functions.py	(revision 5ae8c20848b87cd770d30441da1385f31aed0dbe)
+++ b/Workflow_Programs/Supplemental_Sensor_Graph_Functions.py	(date 1727745964417)
@@ -40,7 +40,8 @@
 		# Input layer with automatic quantization scaling
 		self.layers = nn.ModuleList([
 			qnn.QuantLinear(
-				input_dim, units,
+				in_features=input_dim,  # Correctly using input_dim here
+				out_features=units,
 				bias=True,
 				weight_bit_width=weight_bit_width,
 				weight_quant=Int8WeightPerTensorFixedPoint,
@@ -54,7 +55,8 @@
 		for _ in range(1, layers):
 			self.layers.append(
 				qnn.QuantLinear(
-					units, units,
+					in_features=units,  # Output of the previous layer becomes input to the next
+					out_features=units,
 					bias=True,
 					weight_bit_width=weight_bit_width,
 					weight_quant=Int8WeightPerTensorFixedPoint,
@@ -64,9 +66,10 @@
 			self.activations.append(quant_activation_class(bit_width=act_bit_width))
 			self.dropouts.append(nn.Dropout(dropout_rate))
 		
-		# Output layer
+		# Output layer: You want 1 output (regression problem)
 		self.output_layer = qnn.QuantLinear(
-			units, 1,
+			in_features=units,  # Input to the output layer is the number of units in the last hidden layer
+			out_features=1,  # Output a single value (e.g., calibrated sensor value)
 			bias=True,
 			weight_bit_width=weight_bit_width,
 			weight_quant=Int8WeightPerTensorFixedPoint,
